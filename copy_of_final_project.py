# -*- coding: utf-8 -*-
"""Copy of Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gW0Y43fcv0-kl1EAT9UuWVqBJP9L2s0P

# Data Dictionary

*   Country Name- Countries around the world (Happiness, Suicide, GDP, Median and Average Income Data)
*   Ladder Score- Happiness ranking per country (Happiness Data)
*   Total Suicide Rate- Suicides per 100,000 people for Males and Females per country (Suicide Data)

# Target Variable

Our target variable is median per capita income

# Literature Review

The problem we are trying to solve is how ladder score and suicide rate affect median income per capita. The goal is to find out any relation or connections between ladder score and suicide rate with our target variable. We will also consider how other variables affect median income per capita as part of world happiness as a whole.

*What is Income Per Capita?*: https://www.thebalance.com/income-per-capita-calculation-and-u-s-statistics-3305852

*Mapped: Global Happiness Levels in 2021*: https://www.visualcapitalist.com/mapped-global-happiness-levels-in-2021/

*Happiness and Life Satisfaction*: https://ourworldindata.org/happiness-and-life-satisfaction

*Suicide and Happiness*: https://voxdev.org/topic/public-economics/suicide-and-happiness

*Suicide, age, and wellbeing: an empirical investigation*: http://www.princeton.edu/~accase/downloads/Suicide%20aging%20and%20wellbeing%20boulders%20revised%20June%208%202015.pdf

*Does Economic Development Influence Suicide Rates? An Empirical Analysis:* https://uca.edu/cahss/files/2020/07/03-King-CLA-2020.pdf

*Measuring Socioeconomic Status and Subjective Social Status:* https://www.apa.org/pi/ses/resources/class/measuring-status

*High income improves evaluation of life but not emotional well-being:* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944762/

*Beyond GDP: Economics and Happiness*:https://econreview.berkeley.edu/beyond-gdp-economics-and-happiness/

*Relative Status and Well-Beign*: Evidence from U.S Suicide Deaths: https://www.frbsf.org/economic-research/files/wp07-12bk.pdf

# Data Merging
"""

# import necessary modules
import pandas as pd
import numpy as np

# mount drive
from google.colab import drive
drive.mount('/content/drive')

"""First let's upload all of our datasets."""

# world happiness data
df_wh = pd.read_excel('/content/drive/MyDrive/Summer Analytics Program/Final Project/WHR2021.xls')
df_wh

# gun violence data
df_gv = pd.read_csv('/content/drive/MyDrive/Summer Analytics Program/Final Project/GunViolence.csv')
df_gv

# suicide rates data
df_suic = pd.read_csv('/content/drive/MyDrive/Summer Analytics Program/Final Project/SuicideRate.csv')
df_suic

# cancer rates data
df_cancer = pd.read_csv('/content/drive/MyDrive/Summer Analytics Program/Final Project/CancerRates.csv')
df_cancer

# gdp dataset
df_gdp = pd.read_excel('/content/drive/MyDrive/Summer Analytics Program/Final Project/GDP.xlsx')
df_gdp

# median income dataset
df_income = pd.read_csv('/content/drive/MyDrive/Summer Analytics Program/Final Project/MedianIncome.csv')
df_income

"""Quick recap! What are all of our datasets called? 


*   World Happiness Report: df_wh
*   Gun Violence Dataset: df_gv
*   Suicide Dataset: df_suic
*   Cancer Rates Dataset: df_cancer
*   GDP Dataset: df_gdp
*   Median Income Dataset: df_income

Next step is defining our primary keys for each dataset. They are listed below: 

*   df_wh: Country_name
*   df_gv: country
*   df_suic: country
*   df_cancer: country
*   df_gdp: Country
*   df_income: country

Now let's merge our datasets.
"""

# merge world happiness & gun violence. new dataset name tmp
tmp = pd.merge(left=df_wh,
               right = df_gv,
               left_on = 'Country name',
               right_on = 'country',
               how='outer')
tmp

# merge tmp and gdp. new dataset tmp2
tmp2 = pd.merge(left=tmp,
               right = df_gdp,
               left_on = 'Country name',
               right_on = 'Country',
               how='outer')
tmp2

# merge tmp2 and suicide. new dataset tmp3
tmp3 = pd.merge(left=tmp2,
               right = df_suic,
               left_on = 'Country name',
               right_on = 'Country',
               how='outer')
tmp3

# merge tmp3 and cancer. new dataset tmp4
tmp4 = pd.merge(left=tmp3,
               right = df_cancer,
               left_on = 'Country name',
               right_on = 'country',
               how='outer')
tmp4

# merge tmp4 and income. new dataset tmp5
df = pd.merge(left=tmp4,
               right = df_income,
               left_on = 'Country name',
               right_on = 'country',
               how='outer')

df

# get back to our original 149 rows
df = df[df['Country name'].notna()]
df

"""Now that we have our merged dataset let's drop the extra "country" columns so there is only "country name remaining."""

df.drop(columns=['country_x'], inplace=True)
df.head()

df.drop(columns=['Country_x'], inplace=True)
df.head()

df.drop(columns=['country_y'], inplace=True)
df.head()

df.drop(columns=['country'], inplace=True)

df.drop(columns=['Country_y'], inplace=True)

# check that all additional country columns are dropped
df

# find missing data
df.info()

# use median interpolation to get rid of null values
x = df.fillna(df.median())
x.describe()

# check 
x.info()

# let's drop unwanted columns
x =x.drop(['upperwhisker', 'lowerwhisker',
           '2021 Population', 'pop2021_x', 'pop2021_y',
           'Ladder score in Dystopia', 'Dystopia + residual',
           'medianHouseholdIncome', 'medianAnnualIncome'], axis=1)

#check that columns dropped
x.info()

# our final, merged dataset!
x

# exporting final dataset so we can upload it to Tableau
x.to_excel('/content/drive/MyDrive/Untitled spreadsheet.xlsx')

"""# Exploratory Data Analysis"""

# functions for EDA
import matplotlib.pyplot as plt
from pylab import *
import seaborn as sns

# KDE plot of target variable median income per capita
sns.kdeplot(df['medianPerCapitaIncome'], color="limegreen", shade=True)
fig = plt.gcf()
fig.set_size_inches(12, 8)
title('Median Income Per Capita by Country')
ylabel('Density')
show()

# correlation matrix heatmap of final dataset
plt.figure(figsize=(24, 12))
heatmap = sns.heatmap(x.corr(), vmin=-1, vmax=1, annot=True, cmap='RdPu')
heatmap.set_title('Correlation Matrix Heatmap', fontdict={'fontsize':18}, pad=12);
show()

sns.scatterplot(data=x, x= "Ladder score", y= "medianPerCapitaIncome", hue="Total Suicide Rate", palette="crest", size="Total Suicide Rate", sizes=(40, 150))
fig = plt.gcf()
fig.set_size_inches(15, 9)
plt.legend(loc='upper left', title='Total Suicide Rate')
show()

sns.scatterplot(data=x, x= "Total Suicide Rate", y= "medianPerCapitaIncome", hue="Ladder score", palette="crest", size="Ladder score", sizes=(25, 100))
fig = plt.gcf()
fig.set_size_inches(25, 10)
plt.legend(loc='upper left', title='Ladder Score')
show()

# correlation table of final set 
x.corr()

#description table of final data 
x.describe()

#Creating a flag to see how many countries have median per capita income higher than 4500
x['Flag_MedianPerCapitaIncome'] = np.where(x['medianPerCapitaIncome'] > 4500,
                                 1, # if true, 1 - high median per capita income 
                                 0) # if false, 0 - low median per capita income

x3= x[x['medianPerCapitaIncome'] > 4500] 
x3

x3.shape

"""# Data Splitting"""

# drop categorical columns

x_mod = x.drop(['Country name', 'Regional indicator'], axis=1)

# check
x_mod.info()

# import function for splitting
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import  MinMaxScaler
from sklearn.preprocessing import StandardScaler

# target variable is Y
# this is 'medianPerCapitalIncome'
Y = x_mod['medianPerCapitaIncome']
print(Y.shape)

# everything else is X 
# drop 'medianPerCapitalIncome'
X = x_mod.drop('medianPerCapitaIncome', axis=1)
print(X.shape)

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    test_size = 0.2,
                                                    shuffle = True,
                                                    random_state = 2538651)

# check work 
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# convert these to numpy arrays
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

"""# Full Model"""

scaler = MinMaxScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test)

# functions for regression modeling
from sklearn.ensemble import RandomForestRegressor # RFR
from sklearn.linear_model import LinearRegression #LR

# regression error metrics
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# define RFR
RFR = RandomForestRegressor()
RFR = RFR.fit(X_train, y_train)

# store RFR predictions
train_preds_RFR = RFR.predict(X_train) 
test_preds_RFR = RFR.predict(X_test)

# R2 for Random Forest Regression (RFR)
print("This is train R2 (RFR):", r2_score(y_train, train_preds_RFR))
print("This is test R2 (RFR):", r2_score(y_test, test_preds_RFR))

# MAE for RFR
trainMAE_RFR = mean_absolute_error(y_train, train_preds_RFR)
print("This is trainMAE (RFR):", trainMAE_RFR) 
testMAE_RFR = mean_absolute_error(y_test, test_preds_RFR)
print("This is testMAE (RFR):", testMAE_RFR)

#MSE for RFR
trainMSE_RFR = mean_squared_error(y_train, train_preds_RFR)
print("This is trainMSE (RFR):", trainMSE_RFR)
testMSE_RFR = mean_squared_error(y_test, test_preds_RFR)
print("This is testMSE (RFR):", testMSE_RFR)

# a quick scatterplot for train results
# let's see how it fit
figure(figsize=(8, 6))
scatter(x=y_train, y=train_preds_RFR)
plot([0, 50], [0, 50]) 
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Train Results (RFR)')
show()

# a quick scatterplot for test results

figure(figsize=(8, 6))
scatter(x=y_test, y=test_preds_RFR)
plot([0, 50], [0, 50])
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Test Results (RFR)')
show()

clrfr = RFR # just update the title!

result = permutation_importance(clrfr, X_test, y_test, n_repeats=10,
                                random_state=42)
perm_sorted_idx = result.importances_mean.argsort()

tree_importance_sorted_idx = np.argsort(clrfr.feature_importances_)
tree_indices = np.arange(0, len(clrfr.feature_importances_)) + 0.5

fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))
ax1.boxplot(result.importances[perm_sorted_idx].T, vert=False,
            labels=X.columns[perm_sorted_idx])
fig.suptitle('RFR Feature Importance', y=1.05)
fig.tight_layout()
plt.show()

# make a variable to store the general model
LR = LinearRegression()
# fit the model - one line of code
LR = LR.fit(X_train, y_train)

# store the predictions
train_preds_LR = LR.predict(X_train) 
test_preds_LR = LR.predict(X_test)

# R2 for Random Forest Regression (LR)
print("This is train R2 (LR):", r2_score(y_train, train_preds_LR))
print("This is test R2 (LR):", r2_score(y_test, test_preds_LR))

# MAE (LR)
trainMAE_LR = mean_absolute_error(y_train, train_preds_LR)
print("This is trainMAE (LR):", trainMAE_LR) # train
testMAE_LR = mean_absolute_error(y_test, test_preds_LR)
print("This is testMAE (LR):", testMAE_LR) # test

# MSE (LR)
trainMSE_LR = mean_squared_error(y_train, train_preds_LR)
print("This is trainMSE (LR):", trainMSE_LR)
testMSE_LR = mean_squared_error(y_test, test_preds_LR)
print("This is testMSE (LR):", testMSE_LR)

# a quick scatterplot for train results
# let's see how it fit
figure(figsize=(8, 6))
scatter(x=y_train, y=train_preds_LR)
plot([0, 50], [0, 50], '--k') # 45 degree line
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Train Results (LR)')
show()
show()

# a quick scatterplot for test results
# let's see how it fit
figure(figsize=(8, 6))
scatter(x=y_test, y=test_preds_LR)
plot([0, 50], [0, 50], '--k') # 45 degree line
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Test Results (LR)')
show()

from sklearn.inspection import permutation_importance

clr = LR

result = permutation_importance(clr, X_test, y_test, n_repeats=10,
                                random_state=42)
perm_sorted_idx = result.importances_mean.argsort()

fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))
ax1.boxplot(result.importances[perm_sorted_idx].T, vert=False,
            labels=X.columns[perm_sorted_idx])
fig.suptitle('LR Feature Importance', y=1.05) 
fig.tight_layout()
plt.show()

"""# Reduced Model"""

X2 = x_mod[['GDP', 'femaleCancerRate', 'Ladder score', 'Female Rate', 'Standard error of ladder score', 
            'GDP Growth', 'cancerRate', 'Perceptions of corruption', 'Healthy life expectancy', 'Total Suicide Rate']]

X2.shape
#only 10 columns

print(X2_train.shape, X2_test.shape, y_train.shape, y_test.shape)

X2_train = np.array(X_train)
X2_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

print("This is train R2 (RFR):", r2_score(y_train, train_preds_RFR))
print("This is test R2 (RFR):", r2_score(y_test, test_preds_RFR))

trainMAE_LR = mean_absolute_error(y_train, train_preds_RFR)
print("This is trainMAE (RFR):", trainMAE_LR) # train
testMAE_LR = mean_absolute_error(y_test, test_preds_RFR)
print("This is testMAE (RFR):", testMAE_LR) # test

trainMSE_LR = mean_squared_error(y_train, train_preds_LR)
print("This is trainMSE (LR):", trainMSE_LR)
testMSE_LR = mean_squared_error(y_test, test_preds_LR)
print("This is testMSE (LR):", testMSE_LR)

# a quick scatterplot for test results
# let's see how it fit
figure(figsize=(8, 6))
scatter(x=y_test, y=test_preds_LR)
plot([0, 50], [0, 50], '--k') 
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Test Results (LR)')
show()

# a quick scatterplot for train results
# let's see how it fit
figure(figsize=(8, 6))
scatter(x=y_train, y=train_preds_LR)
plot([0, 50], [0, 50], '--k') 
axis('tight')
xlabel('True Income ($1000s)')
ylabel('Predicted Income ($1000s)')
suptitle('Train Results (LR)')
show()
show()

clr2 = LR

result = permutation_importance(clr2, X2_test, y_test, n_repeats=10, random_state=42)
perm_sorted_idx = result.importances_mean.argsort()

fig, ax1 = plt.subplots(1, 1, figsize=(0, 10))
ax1.boxplot(result.importances[perm_sorted_idx].T, vert=False,
            labels=X2.columns[perm_sorted_idx])
fig.suptitle('LR Feature Importance', y=1.05) 
fig.tight_layout()
plt.show()

"""# Analysis

## Results:

Some of our observations were deleted to eliminate error metrics in the report of median per capita income. We believe that we have achieved more accuracy using our full model compared to our reduced model. The model that we think has provided better results was the full model because we can clearly see a strong positive correlation between True Income and Predicted Income by looking at the Random Forest Regression portion. In comparison with other scatterplots in our reduced models, we can clearly see that True Income and Predicted Income show a weak correlation. Error metrics is a way to measure the error of a forecasting model meaning that it is providing a way for predicter to quantitatively compare the performance or accuracy of competing models. In this study, we have used MAE and MSE error metrics to analyze our full and reduce models.

# Discussion

Our results have shown that ladder score do have an effect on median per capita income. By looking for certain countries in our data results, we can see that Finland is one of the countries that show a positive correlation. In a study done in 2018, Finland ranked as one of the happiest country on earth. We can clearly say that happiness seem to go together with income. By looking at other studies on how GDP has a relationship with ladder scores, we can see that there is an effect as countries have a higher GDP, the population seems to have a higher happy score. If we focus on Finland, we can clearly see on the study done in 2021 about happiness level; Finland keeps scoring as the most happy country worldwide. Furthermore,  analyzing the relationship between suicide rate and happiness we can see that there is a weak correlation. Looking at a study about suicide and life evaluation, we can see that there is not correlation. We can see a correlation on the amout of weatlh and ladder score. Once again, we can see that ladder score and income keep having a strong correlation. 





*https://www.visualcapitalist.com/mapped-global-happiness-levels-in-2021/*

*https://voxdev.org/topic/public-economics/suicide-and-happiness*

*https://econreview.berkeley.edu/beyond-gdp-economics-and-happiness/*

*https://ourworldindata.org/happiness-and-life-satisfaction*
"""